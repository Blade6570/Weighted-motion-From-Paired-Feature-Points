<?xml version="1.0" encoding="utf-8"?>
<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Strict//EN"
"http://www.w3.org/TR/xhtml1/DTD/xhtml1-strict.dtd">
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en">
<head>
<!-- 2021-04-18 Sun 00:18 -->
<meta http-equiv="Content-Type" content="text/html;charset=utf-8" />
<meta name="viewport" content="width=device-width, initial-scale=1" />
<title>Single Source One Shot Reenactment using Weighted motion From Paired Feature Points</title>
<meta name="generator" content="Org mode" />
<meta name="author" content="Soumya Tripathy" />
<style type="text/css">
 <!--/*--><![CDATA[/*><!--*/
  .title  { text-align: center;
             margin-bottom: .2em; }
  .subtitle { text-align: center;
              font-size: medium;
              font-weight: bold;
              margin-top:0; }
  .todo   { font-family: monospace; color: red; }
  .done   { font-family: monospace; color: green; }
  .priority { font-family: monospace; color: orange; }
  .tag    { background-color: #eee; font-family: monospace;
            padding: 2px; font-size: 80%; font-weight: normal; }
  .timestamp { color: #bebebe; }
  .timestamp-kwd { color: #5f9ea0; }
  .org-right  { margin-left: auto; margin-right: 0px;  text-align: right; }
  .org-left   { margin-left: 0px;  margin-right: auto; text-align: left; }
  .org-center { margin-left: auto; margin-right: auto; text-align: center; }
  .underline { text-decoration: underline; }
  #postamble p, #preamble p { font-size: 90%; margin: .2em; }
  p.verse { margin-left: 3%; }
  pre {
    border: 1px solid #ccc;
    box-shadow: 3px 3px 3px #eee;
    padding: 8pt;
    font-family: monospace;
    overflow: auto;
    margin: 1.2em;
  }
  pre.src {
    position: relative;
    overflow: auto;
    padding-top: 1.2em;
  }
  pre.src:before {
    display: none;
    position: absolute;
    background-color: white;
    top: -10px;
    right: 10px;
    padding: 3px;
    border: 1px solid black;
  }
  pre.src:hover:before { display: inline; margin-top: 14px;}
  /* Languages per Org manual */
  pre.src-asymptote:before { content: 'Asymptote'; }
  pre.src-awk:before { content: 'Awk'; }
  pre.src-C:before { content: 'C'; }
  /* pre.src-C++ doesn't work in CSS */
  pre.src-clojure:before { content: 'Clojure'; }
  pre.src-css:before { content: 'CSS'; }
  pre.src-D:before { content: 'D'; }
  pre.src-ditaa:before { content: 'ditaa'; }
  pre.src-dot:before { content: 'Graphviz'; }
  pre.src-calc:before { content: 'Emacs Calc'; }
  pre.src-emacs-lisp:before { content: 'Emacs Lisp'; }
  pre.src-fortran:before { content: 'Fortran'; }
  pre.src-gnuplot:before { content: 'gnuplot'; }
  pre.src-haskell:before { content: 'Haskell'; }
  pre.src-hledger:before { content: 'hledger'; }
  pre.src-java:before { content: 'Java'; }
  pre.src-js:before { content: 'Javascript'; }
  pre.src-latex:before { content: 'LaTeX'; }
  pre.src-ledger:before { content: 'Ledger'; }
  pre.src-lisp:before { content: 'Lisp'; }
  pre.src-lilypond:before { content: 'Lilypond'; }
  pre.src-lua:before { content: 'Lua'; }
  pre.src-matlab:before { content: 'MATLAB'; }
  pre.src-mscgen:before { content: 'Mscgen'; }
  pre.src-ocaml:before { content: 'Objective Caml'; }
  pre.src-octave:before { content: 'Octave'; }
  pre.src-org:before { content: 'Org mode'; }
  pre.src-oz:before { content: 'OZ'; }
  pre.src-plantuml:before { content: 'Plantuml'; }
  pre.src-processing:before { content: 'Processing.js'; }
  pre.src-python:before { content: 'Python'; }
  pre.src-R:before { content: 'R'; }
  pre.src-ruby:before { content: 'Ruby'; }
  pre.src-sass:before { content: 'Sass'; }
  pre.src-scheme:before { content: 'Scheme'; }
  pre.src-screen:before { content: 'Gnu Screen'; }
  pre.src-sed:before { content: 'Sed'; }
  pre.src-sh:before { content: 'shell'; }
  pre.src-sql:before { content: 'SQL'; }
  pre.src-sqlite:before { content: 'SQLite'; }
  /* additional languages in org.el's org-babel-load-languages alist */
  pre.src-forth:before { content: 'Forth'; }
  pre.src-io:before { content: 'IO'; }
  pre.src-J:before { content: 'J'; }
  pre.src-makefile:before { content: 'Makefile'; }
  pre.src-maxima:before { content: 'Maxima'; }
  pre.src-perl:before { content: 'Perl'; }
  pre.src-picolisp:before { content: 'Pico Lisp'; }
  pre.src-scala:before { content: 'Scala'; }
  pre.src-shell:before { content: 'Shell Script'; }
  pre.src-ebnf2ps:before { content: 'ebfn2ps'; }
  /* additional language identifiers per "defun org-babel-execute"
       in ob-*.el */
  pre.src-cpp:before  { content: 'C++'; }
  pre.src-abc:before  { content: 'ABC'; }
  pre.src-coq:before  { content: 'Coq'; }
  pre.src-groovy:before  { content: 'Groovy'; }
  /* additional language identifiers from org-babel-shell-names in
     ob-shell.el: ob-shell is the only babel language using a lambda to put
     the execution function name together. */
  pre.src-bash:before  { content: 'bash'; }
  pre.src-csh:before  { content: 'csh'; }
  pre.src-ash:before  { content: 'ash'; }
  pre.src-dash:before  { content: 'dash'; }
  pre.src-ksh:before  { content: 'ksh'; }
  pre.src-mksh:before  { content: 'mksh'; }
  pre.src-posh:before  { content: 'posh'; }
  /* Additional Emacs modes also supported by the LaTeX listings package */
  pre.src-ada:before { content: 'Ada'; }
  pre.src-asm:before { content: 'Assembler'; }
  pre.src-caml:before { content: 'Caml'; }
  pre.src-delphi:before { content: 'Delphi'; }
  pre.src-html:before { content: 'HTML'; }
  pre.src-idl:before { content: 'IDL'; }
  pre.src-mercury:before { content: 'Mercury'; }
  pre.src-metapost:before { content: 'MetaPost'; }
  pre.src-modula-2:before { content: 'Modula-2'; }
  pre.src-pascal:before { content: 'Pascal'; }
  pre.src-ps:before { content: 'PostScript'; }
  pre.src-prolog:before { content: 'Prolog'; }
  pre.src-simula:before { content: 'Simula'; }
  pre.src-tcl:before { content: 'tcl'; }
  pre.src-tex:before { content: 'TeX'; }
  pre.src-plain-tex:before { content: 'Plain TeX'; }
  pre.src-verilog:before { content: 'Verilog'; }
  pre.src-vhdl:before { content: 'VHDL'; }
  pre.src-xml:before { content: 'XML'; }
  pre.src-nxml:before { content: 'XML'; }
  /* add a generic configuration mode; LaTeX export needs an additional
     (add-to-list 'org-latex-listings-langs '(conf " ")) in .emacs */
  pre.src-conf:before { content: 'Configuration File'; }

  table { border-collapse:collapse; }
  caption.t-above { caption-side: top; }
  caption.t-bottom { caption-side: bottom; }
  td, th { vertical-align:top;  }
  th.org-right  { text-align: center;  }
  th.org-left   { text-align: center;   }
  th.org-center { text-align: center; }
  td.org-right  { text-align: right;  }
  td.org-left   { text-align: left;   }
  td.org-center { text-align: center; }
  dt { font-weight: bold; }
  .footpara { display: inline; }
  .footdef  { margin-bottom: 1em; }
  .figure { padding: 1em; }
  .figure p { text-align: center; }
  .equation-container {
    display: table;
    text-align: center;
    width: 100%;
  }
  .equation {
    vertical-align: middle;
  }
  .equation-label {
    display: table-cell;
    text-align: right;
    vertical-align: middle;
  }
  .inlinetask {
    padding: 10px;
    border: 2px solid gray;
    margin: 10px;
    background: #ffffcc;
  }
  #org-div-home-and-up
   { text-align: right; font-size: 70%; white-space: nowrap; }
  textarea { overflow-x: auto; }
  .linenr { font-size: smaller }
  .code-highlighted { background-color: #ffff00; }
  .org-info-js_info-navigation { border-style: none; }
  #org-info-js_console-label
    { font-size: 10px; font-weight: bold; white-space: nowrap; }
  .org-info-js_search-highlight
    { background-color: #ffff00; color: #000000; font-weight: bold; }
  .org-svg { width: 90%; }
  /*]]>*/-->
</style>
<link id ="pagestyle"  rel="stylesheet" type="text/css" href="./org.css"/>
<script type="text/javascript">
// @license magnet:?xt=urn:btih:e95b018ef3580986a04669f1b5879592219e2a7a&dn=public-domain.txt Public Domain
<!--/*--><![CDATA[/*><!--*/
     function CodeHighlightOn(elem, id)
     {
       var target = document.getElementById(id);
       if(null != target) {
         elem.classList.add("code-highlighted");
         target.classList.add("code-highlighted");
       }
     }
     function CodeHighlightOff(elem, id)
     {
       var target = document.getElementById(id);
       if(null != target) {
         elem.classList.remove("code-highlighted");
         target.classList.remove("code-highlighted");
       }
     }
    /*]]>*///-->
// @license-end
</script>
</head>
<body>
<div id="content">
<h1 class="title">Single Source One Shot Reenactment using Weighted motion From Paired Feature Points
<br />
<span class="subtitle"><a href="https://blade6570.github.io/soumyatripathy/">Soumya Tripathy</a>, <a href="https://users.aalto.fi/~kannalj1/">Juho Kannala</a> and <a href="http://esa.rahtu.fi/">Esa Rahtu</a> <br />
Code coming soon</span>
</h1>
  <style>

      .wrapper {
        background-color: cyan;
        border: none;
        color: white;
        padding: 0px 0px;
        text-align: center;
        text-decoration: none;
        display: inline-block;
        font-size: 20px;
        margin: 1px 100px;
        cursor: pointer;
	position: relative;
	border-radius: 5px;
	display: block;
	width: 230px;
	margin: 0px auto 0px auto;
      }

    </style>

<div class="wrapper">
  <a class="button" href="https://arxiv.org/abs/2104.03117">Paper</a>>
  <a class="button" href="https://www.youtube.com/watch?v=fJn5WU01ITc">Video</a>>
  <a class="button" href="https://github.com/">Code</a>
</div>


<div id="outline-container-org19600cb" class="outline-2">
<h2 id="org19600cb"><span class="section-number-2">1</span> Abstract</h2>
<div class="outline-text-2" id="text-1">
<div class="left" id="org19b2bb1">
<p>
Image reenactment is a task where the target object in
the source image imitates the motion represented in the driving image. One of the most common reenactment tasks is
face image animation. The major challenge in the current
face reenactment approaches is to distinguish between facial motion and identity. For this reason, the previous models struggle to produce high-quality animations if the driving and source identities are different (cross-person reenactment). We propose a new (face) reenactment model
that learns shape-independent motion features in a selfsupervised setup. The motion is represented using a set of
paired feature points extracted from the source and driving
images simultaneously. The model is generalised to multiple reenactment tasks including faces and non-face objects
using only a single source image. The extensive experiments
show that the model faithfully transfers the driving motion
to the source while retaining the source identity intact.<br />
<br />
<br />
<br />
</p>

</div>

<iframe style="display: block; margin: auto;" width="900" height="600" src="https://www.youtube.com/embed/fJn5WU01ITc" frameborder="0" allowfullscreen></iframe>
<p>
<br />
<br />
</p>
</div>
</div>
<div id="outline-container-org9aea89c" class="outline-2">
<h2 id="org9aea89c"><span class="section-number-2">2</span> Key Idea</h2>
<div class="outline-text-2" id="text-2">
<p>
Face landmarks or keypoint based models<sup><a id="fnr.1" class="footref" href="#fn.1">1</a></sup><sup>, </sup><sup><a id="fnr.2" class="footref" href="#fn.2">2</a></sup> generate high-quality talking heads for self reenactment, but often fail in cross-person reenactment where the source and driving image have different identities. The main reason is that landmarks/keypoints are person-specific and carry facial shape information in terms of pose independent head geometry. Any differences of shape between source and driving heads are reflected in the facial motion (through landmarks or keypoints) and lead to a talking head that can not faithfully retain the identity of the source’s person. This effect can be seen in Figure 1 for faces and in Figure 3 for non-face objects using a keypoint based reenactment model like FOM <sup><a id="fnr.1.100" class="footref" href="#fn.1">1</a></sup>. Furthermore, these models use each keypoint independently to affect the motion of its neighborhood pixels which makes the output highly dependent on the quality of the keypoints or land-
marks. Any noisy keypoint prediction may severely distort the facial shape and thereby generate low-quality talking heads of the source as shown in Figure 1.
</p>


<div id="orga9c49ff" class="figure">
<p><img src="./g398.png" alt="g398.png" style=":left;margin:0px 0px 0px 0px;" width="900px" />
</p>
<p><span class="figure-number">Figure 1: </span>. Illustration of drawbacks in keypoints/landmarks based reenactment models. In both cases, the reenactment is performed using FOM and the keypoints are drawn on the source and driving images. In Reenactment-1, the head structure difference between the source and driving is reflected in the output (bottom image) as the source’s facial structure and identity are distorted. In the Reenactment-2, one of the key points (in the red box) is slightly displaced manually from its original position to show its effect on the output. The degradation in the output quality shows the overall system performance is highly dependant on the keypoint detectors.</p>
</div>

<p>
Considering these issues, we propose a new (face) reenactment model that learns shape-independent motion features in a self-supervised setup. The motion is represented using a set of paired feature points extracted from the source and driving images simultaneously. The model is generalised to multi-
ple reenactment tasks including faces and non-face objects using only a single source image. The complete block diagram of our model is given in Figure 3 and demo is presented in the youtube video attached in this page. The extensive experiments show that the model faithfully transfers the driving motion to the source while retaining the source identity intact. For further details please refer to our paper.
</p>


<div id="orgc82245c" class="figure">
<p><img src="./block_dig.png" alt="block_dig.png" style=":left;margin:0px 10px 0px 10px;" width="1000px" />
</p>
<p><span class="figure-number">Figure 2: </span>The complete block diagram of the proposed reenactment model. It processes the source and driving images in five steps as 1. Encoding the images using image embedder, 2. Extracting paired-feature-points using a transformer, 3. Estimating the motion from paired-feature-points, 4. Converting the motion to the warping field and 5. Using the source image with the warping field in the generator to produce the final output. The transformer module is expanded at the bottom to showcase its building blocks in detail.</p>
</div>
</div>
</div>

<div id="outline-container-org0166dbb" class="outline-2">
<h2 id="org0166dbb"><span class="section-number-2">3</span> Reenacting non-face objects</h2>
<div class="outline-text-2" id="text-3">
<p>
The proposed formulation does not make any assumptions on the reenacted object type. Therefore, the same model can be also trained without modifications to reenact other objects besides faces.
</p>


<div id="orgfd83e24" class="figure">
<p><img src="./mgif.png" alt="mgif.png" style=":left;margin:0px 10px 0px 10px;" width="1000px" />
</p>
<p><span class="figure-number">Figure 3: </span>Qualitative comparison of proposed model with FOM on a. Tai-chi-HD, and b. MGif datasets. Our model keeps the source shape and driver’s motion intact at the output unlike FOM. More results can be seen in the video.</p>
</div>
</div>

<div id="outline-container-org9455c55" class="outline-3">
<h3 id="org9455c55"><span class="section-number-3">3.1</span> Some Examples on Bair action-conditioned robot pushing dataset</h3>
<div class="outline-text-3" id="text-3-1">
<style>
.center {
  display: block;
  margin-left: auto;
  margin-right: auto;
  width: 50%;
}
</style>

<IMG SRC="./04_12_text.gif" class="center">
<IMG SRC="./19_11_text.gif" class="center">
<IMG SRC="./01_15_text.gif" class="center">
</div>
</div>
</div>
<div id="outline-container-orgc25d477" class="outline-2">
<h2 id="orgc25d477"><span class="section-number-2">4</span> Citation</h2>
<div class="outline-text-2" id="text-4">
<p>
If you find this useful in your research work then please cite us as:
</p>

<div class="org-src-container">
<pre class="src src-bibtex"><span style="color: #D9DAA2;">@misc</span>{<span style="color: #339CDB;">tripathy2021single</span>,
      <span style="color: #85DDFF;">title</span>={Single Source One Shot Reenactment using Weighted motion From Paired Feature Points},
      <span style="color: #85DDFF;">author</span>={Soumya Tripathy and Juho Kannala and Esa Rahtu},
      <span style="color: #85DDFF;">year</span>={2021},
      <span style="color: #85DDFF;">eprint</span>={2104.03117},
      <span style="color: #85DDFF;">archivePrefix</span>={arXiv},
      <span style="color: #85DDFF;">primaryClass</span>={cs.CV}}
</pre>
</div>
</div>
</div>

<div id="outline-container-org757ae5e" class="outline-2">
<h2 id="org757ae5e"><span class="section-number-2">5</span> Other related works</h2>
<div class="outline-text-2" id="text-5">
<div class="org-src-container">
<pre class="src src-bibtex"><span style="color: #D9DAA2;">@InProceedings</span>{<span style="color: #339CDB;">Tripathy_2021_WACV</span>,
<span style="color: #85DDFF;">author</span> = {Tripathy, Soumya and Kannala, Juho and Rahtu, Esa},
<span style="color: #85DDFF;">title</span> = {FACEGAN: Facial Attribute Controllable rEenactment GAN},
<span style="color: #85DDFF;">booktitle</span> = {Proceedings of the IEEE/CVF Winter Conference on Applications of Computer Vision (WACV)},
<span style="color: #85DDFF;">year</span> = {2021}
}
<span style="color: #D9DAA2;">@InProceedings</span>{<span style="color: #339CDB;">Tripathy_2020_WACV</span>,
<span style="color: #85DDFF;">author</span> = {Tripathy, Soumya and Kannala, Juho and Rahtu, Esa},
<span style="color: #85DDFF;">title</span> = {ICface: Interpretable and Controllable Face Reenactment Using GANs},
<span style="color: #85DDFF;">booktitle</span> = {Proceedings of the IEEE/CVF Winter Conference on Applications of Computer Vision (WACV)},
<span style="color: #85DDFF;">month</span> = {March},
<span style="color: #85DDFF;">year</span> = {2020}
}

</pre>
</div>
</div>
</div>
<div id="footnotes">
<h2 class="footnotes">Footnotes: </h2>
<div id="text-footnotes">

<div class="footdef"><sup><a id="fn.1" class="footnum" href="#fnr.1">1</a></sup> <div class="footpara"><p class="footpara">
Siarohin, A., Lathuilière, S., Tulyakov, S., Ricci, E., Sebe, N.: First order motion model for image animation. In: Conference on Neural Information Processing Systems (NeurIPS). (2019)
</p></div></div>

<div class="footdef"><sup><a id="fn.2" class="footnum" href="#fnr.2">2</a></sup> <div class="footpara"><p class="footpara">
Zakharov, E., Shysheya, A., Burkov, E., Lempitsky, V.: Few-shot adversarial learning of realistic neural talking head models. In: Proceedings of the IEEE International Conference on Computer Vision. (2019) 9459–9468.
</p></div></div>


</div>
</div></div>
<div id="postamble" class="status">
<hr><footer><a href="https://research.tuni.fi/vision/"><img alt="Tampere Logo" style="border-width:0" src="./Tampere-logo.jpg" width="90" height="90"/></a><a rel="license" href="https://research.tuni.fi/vision/"><img alt="aalto Logo" style="border-width:0" src="./download.png" width="150" height="100"/></a><br />Created by Org version 9.3 with Emacs version 27.2 </footer> Last Updated 2021-04-18 Sun 00:18 Created by Soumya Tripathy
</div>
</body>
</html>
